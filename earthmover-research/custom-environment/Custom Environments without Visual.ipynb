{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c91fe66c-a774-4b12-8b15-7f7ae76865a5",
   "metadata": {},
   "source": [
    "# How to Create a Custom Environment\n",
    "In this Jupyter Notebook, we will implement a simple game called `GridWorldEnv` which consists of a 2 dimensional square grid of fixed size. The agent can move vertically or horizontally betweeng grid cells in each timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1aadc-8e13-4729-bfb3-17456e4103e2",
   "metadata": {},
   "source": [
    "### Goal & Basic Information\n",
    "THe goal of the agent is to navigate to a target on the grid that has been placed randomly at the beginning of the episode. Here is basic information about the game:\n",
    "-  Observations provide the location of the target and agent\n",
    "-  There are 4 discrete actions in our environment, corresponding to the movements \"right\", \"up\", \"left\" and \"down\"\n",
    "-  The environment ends (terminates) when the agent has navigated to the grid cell where the target is located\n",
    "-  The agent is only rewarded when it reaches the target, i.e, the reward is one when the agent reaches the target and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef01257-043f-434f-8049-1211f9fc92cb",
   "metadata": {},
   "source": [
    "### Environment `__init__`\n",
    "The custom environment will inheirt from `gymnasium.Env` that defines the structure of the environment.\n",
    "\n",
    "- One of the requirements for an environment is defining the observation and action space, which delcares the general set of possible inputs (actions) and outputs (observations) of the environment\n",
    "\n",
    "As outlined previously, our agent has 4 discrete actions, therefore we use `Discrete(4)` space with 4 options. For our observation, we can imagine our observation looks like `{\"agent\": array([1, 0]), \"target\": array([0, 3])}`\n",
    "\n",
    "This is where the array elements represents the x and y positions of the agent or target. Alternative options for representing the observation is a 2D grid with values representing the agent and target on the grid or a 3D grid with each \"layer\" containing only the agent or target information. \n",
    "\n",
    "Hence, we declare the observation space as `Dict` with the agent and target spaces being a `Box` allowing an array output of an int type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cd19f44-ce73-46ec-953e-ba0990e691fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f31fcfc-3396-453b-9fd9-11dc91e68fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    def __init__(self, size: int = 5):\n",
    "\n",
    "        # size of the square grid\n",
    "        self.size = size\n",
    "\n",
    "        # define the agent and target location; randomly chosen in 'reset' and updated in 'step'\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        # observations are dictionaries with the agent's and the target's location\n",
    "        # each location is encoded as an element of {0, ..., `size` -1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent\": gym.spaces.Box(0, size-1, shape=(2,), dtype=int),\n",
    "                \"target\": gym.spaces.Box(0, size-1, shape=(2,), dtype=int)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # we have 4 actions, corresponding to \"right\", \"up\", \"left\" and \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        # dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]), # right\n",
    "            1: np.array([0, 1]), # up\n",
    "            2: np.array([-1, 0]), # left\n",
    "            3: np.array([0, -1]), # down\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a740e-46ec-4ef8-91d8-362362a5babb",
   "metadata": {},
   "source": [
    "```\n",
    "### Initalizing Grid & Environment\n",
    "self.size = size\n",
    ".  .  .  .  .\n",
    ".  .  .  .  .\n",
    ".  .  .  .  .\n",
    ".  .  .  .  .\n",
    ".  .  .  .  .\n",
    "\n",
    "\n",
    "### Agent & Target Initializtion\n",
    "self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "Grid is initialized, but the agent and target are not yet placed.\n",
    "\n",
    "\n",
    "### Observation Space\n",
    "self.observation_space = gym.spaces.Dict(\n",
    "    {\n",
    "        \"agent\": gym.spaces.Box(0, size-1, shape=(2,), dtype=int),\n",
    "        \"target\": gym.spaces.Box(0, size-1, shape=(2,), dtype=int)\n",
    "    }\n",
    ")\n",
    "The agent observes both its own position and the target's position as [x, y] coordinates in the grid.\n",
    "{\n",
    "    \"agent\": [1, 2],\n",
    "    \"target\": [3, 3]\n",
    "}\n",
    "\n",
    "### Action Space\n",
    "self.action_space = gym.spaces.Discrete(4)\n",
    "A  .  .      # Action 0 (Right): Agent moves to [2, 3].\n",
    ".  A  .      # Action 1 (Up):    Agent moves to [1, 2].\n",
    ".  A  .      # Action 2 (Left):  Agent moves to [2, 1].\n",
    ".  A  .      # Action 3 (Down):  Agent moves to [3, 2].\n",
    "\n",
    "### Action to Direction Mapping\n",
    "self._action_to_direction = {\n",
    "    0: np.array([1, 0]),  # right\n",
    "    1: np.array([0, 1]),  # up\n",
    "    2: np.array([-1, 0]), # left\n",
    "    3: np.array([0, -1]), # down\n",
    "}\n",
    "\n",
    "Action 0 (Right): Add [1, 0] to the current position.\n",
    "[2, 2] + [1, 0] = [3, 2]\n",
    "Action 1 (Up): Add [0, 1] to the current position.\n",
    "[2, 2] + [0, 1] = [2, 3]\n",
    "Action 2 (Left): Add [-1, 0] to the current position.\n",
    "[2, 2] + [-1, 0] = [1, 2]\n",
    "Action 3 (Down): Add [0, -1] to the current position.\n",
    "[2, 2] + [0, -1] = [2, 1]\n",
    "\n",
    "    Up (1)\n",
    "      ↑\n",
    "Left ← A → Right\n",
    "      ↓\n",
    "   Down (3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8ee27-9383-4a9f-b599-46cf67ccdebd",
   "metadata": {},
   "source": [
    "### Consturcting Observations\n",
    "Since we need to compute observations in both `Env.reset()` and `Env.step()`, it is often convenient to have a method `_get_obs` that translates the environment's state into an observation. This isn't mandatory and you can compute the observations in `Env.reset()` and `Env.step()` separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eb391a9-f80a-4d0a-a997-39871ba07868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the location of both the target and the agent\n",
    "def _get_obs(self):\n",
    "    return {\"agent\": self.__agent_location, \"target\": self._target_location}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79329d62-2ee3-4cc3-bae5-395f711f2289",
   "metadata": {},
   "source": [
    "We also implement a similar method to the information returned by `Env.reset()` and `Env.step()`. In this case, we would like to find the manhattan distance between the agent and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ace53f6-b47d-4d48-bc27-cde2fa3ac6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the manhattan distance between the agent and the target\n",
    "def _get_info(self):\n",
    "    return {\n",
    "        \"distance\": np.linalg.norm(\n",
    "            self._agent_location - self._target_location, ord=1\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e949d-a532-4d1e-96d0-855260a221fa",
   "metadata": {},
   "source": [
    "Oftentimes, info will also contain some data that is only available in the `Env.step()` method. In that case, we would need to update the dictionary this is returned by `_get_info` in `Env.step()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d40f65-c2ec-422b-a85c-489482bfb4a7",
   "metadata": {},
   "source": [
    "### Reset Function\n",
    "The purpose of `reset()` is to initiate a new episode for an environment and has two parameters, which are `seed` and `options`. \n",
    "- The seed is used to initialize the random number generator to a deterministic state\n",
    "- Options can be used to specify values used within reset\n",
    "On the first line of reset, you need to call `super().reset(seed=seed)` which will initialize the random number generate `np_random` to use through the rest of the `reset()`\n",
    "\n",
    "Within the custom environment, the `reset()` needs to randomly choose the agent and target's positions. The return tpe of `reset()` is a tuple of the initial observation and auxiliary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34159bab-ad93-4e86-95fd-a61de8cbbfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "\n",
    "    # seeding self.np_random\n",
    "    super().reset(seed=seed)\n",
    "\n",
    "    # choosing the agent's location uniformly at random\n",
    "    self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "    # we sample the target's location randomly until it doesn't coincide with agent's location\n",
    "    self._target_location = self._agent_location\n",
    "    while np.array_equal(self._target_location, self._agent_location):\n",
    "        self._target_location = self.np_random_integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "    # retrieving the observation and information\n",
    "    observation = self._get_obs()\n",
    "    info = self._get_info()\n",
    "\n",
    "    # returning the observation and information\n",
    "    return observation, info "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc466b92-e730-4c39-b9cf-3f0e7b7bcde6",
   "metadata": {},
   "source": [
    "### Step Function\n",
    "The `step()` method usually contains most of the logic for the environment, where it accepts a `action` and computes the state of the environment after applying the action. It then returns:\n",
    "- a tuple of the next observation\n",
    "- the resulting reward\n",
    "- if the environment was terminated\n",
    "- if the environment has truncated and auxiliary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c202a0f-8fa5-4322-a61e-57a1c7621cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, action):\n",
    "\n",
    "    # map the action (element of {0, 1, 2, 3} to the direction the agent walks\n",
    "    direction = self._action_to_direction[action]\n",
    "\n",
    "    # use `np.clip` to make sure agent doesn't leave the grid bounds\n",
    "    self._agent_location = np.clip(self._agent_location + direction, 0, self.size - 1)\n",
    "\n",
    "    # a environment is completed if and only if the agent has reached the target\n",
    "    terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "    truncated = False\n",
    "    reward = 1 if terminated else 0\n",
    "    observation = self._get_obs()\n",
    "    info = self._get_info()\n",
    "\n",
    "    # returning all the details\n",
    "    return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0e6bb-a937-4fba-a60d-021ad268d95d",
   "metadata": {},
   "source": [
    "### Registering & Making the Environment\n",
    "It's more common for environments to be initialized using `gymnasium.make()`. The environment ID consists of three components, two of which are optional:\n",
    "- `gymnasium_env` which is a mandatory name\n",
    "- `v0` optional version naming\n",
    "- an appropariate ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0e253a0-ad01-48e8-9a9f-9a1db361a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id=\"gymnasium_env/GridWorld-v0\",\n",
    "    entry_point=GridWorldEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ece978ae-edd0-4fc3-b5de-f5e981f77bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OrderEnforcing<PassiveEnvChecker<GridWorldEnv<gymnasium_env/GridWorld-v0>>>>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"gymnasium_env/GridWorld-v0\")\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c790cb6-4049-4865-8422-9d9040bba3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<GridWorldEnv<gymnasium_env/GridWorld-v0>>>>>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.make(\"gymnasium_env/GridWorld-v0\", max_episode_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca858271-65e9-4c99-b552-a0f50a5abea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('agent': Box(0, 4, (2,), int64), 'target': Box(0, 4, (2,), int64))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9338985-0997-4362-a5a8-2a49c6de1963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "box-2d",
   "language": "python",
   "name": "box-2d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
